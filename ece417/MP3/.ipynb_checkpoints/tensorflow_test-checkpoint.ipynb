{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from scipy import signal\n",
    "from imageio import imread\n",
    "from random import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_images\n",
    "    # Read in images and makes a list for each set in the form: [images, labels]\n",
    "    # images: np array with dims [N x img_height x img width x num_channels]\n",
    "    # labels: np array with dims [N x 1]. elephant = 0, lionfish = 1\n",
    "    #\n",
    "    # Returns:  train_set: The list [train_images, train_labels]\n",
    "    #           val_set: The list [val_images, val_labels] \n",
    "\n",
    "def load_images():\n",
    "    \n",
    "    sets = ['train', 'val']\n",
    "    \n",
    "    data_sets = []\n",
    "    for dset in sets:\n",
    "        img_path = './bin_dataset/' + dset + '/ele'\n",
    "        ele_list = [imread(os.path.join(img_path, img)) for img in os.listdir(img_path)]\n",
    "\n",
    "        img_path = './bin_dataset/' + dset + '/lio'\n",
    "        lio_list = [imread(os.path.join(img_path, img)) for img in os.listdir(img_path)]\n",
    "\n",
    "        set_images = np.stack(ele_list + lio_list)\n",
    "        N = set_images.shape[0]\n",
    "        labels = np.ones((N,1))\n",
    "        labels[0:int(N/2)] = 0\n",
    "        data_sets.append([set_images, labels])\n",
    "\n",
    "    train_set, val_set = data_sets\n",
    "\n",
    "    print(\"Loaded\", len(train_set[0]), \"training images\")\n",
    "    print(\"Loaded\", len(val_set[0]), \"validation images\")\n",
    "    \n",
    "    return train_set, val_set\n",
    "\n",
    "\n",
    "# batchify\n",
    "    # Inputs:    train_set: List containing images and labels\n",
    "    #            batch size: The desired size of each batch\n",
    "    #\n",
    "    # Returns:   image_batches: A list of shuffled training image batches, each with size batch_size\n",
    "    #            label_batches: A list of shuffled training label batches, each with size batch_size \n",
    "\n",
    "def batchify(train_set, batch_size):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # initialized two lists\n",
    "    image_batches = []\n",
    "    label_batches = []\n",
    "    \n",
    "    shuffle_index = np.arange(len(train_set[0]))\n",
    "    shuffle(shuffle_index)\n",
    "    \n",
    "    image_chunk = [None] * batch_size\n",
    "    label_chunk = [None] * batch_size\n",
    "    for c in range(0, len(shuffle_index), batch_size):\n",
    "        for i in range(batch_size):\n",
    "            image_chunk[i] = train_set[0][shuffle_index[c+i]]\n",
    "            label_chunk[i] = train_set[1][shuffle_index[c+i]]\n",
    "        image_batches.append(np.array(image_chunk))\n",
    "        label_batches.append(np.array(label_chunk))\n",
    "\n",
    "    return image_batches, label_batches\n",
    "\n",
    "def data_transform(data_set):\n",
    "    data, label = data_set\n",
    "    data = data.astype(float)\n",
    "    data /= 256.0\n",
    "    N = len(data)\n",
    "    new_label = np.zeros((N, 2)).astype(int)\n",
    "    new_label[:, 0:1] = (1-label).astype(int)\n",
    "    new_label[:, 1:2] = label.astype(int)\n",
    "    return [data, new_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 training images\n",
      "Loaded 800 validation images\n"
     ]
    }
   ],
   "source": [
    "# Load images and scale them\n",
    "# YOUR CODE HERE\n",
    "train_set, val_set = load_images()\n",
    "train_set = data_transform(train_set)\n",
    "val_set = data_transform(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_2:0\", shape=(4,), dtype=int32) Tensor(\"Const_3:0\", shape=(4,), dtype=int32)\n",
      "Tensor(\"Mul_1:0\", shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Initialize two constants\n",
    "x1 = tf.constant([1,2,3,4])\n",
    "x2 = tf.constant([5,6,7,8])\n",
    "print(x1, x2)\n",
    "# Multiply\n",
    "result = tf.multiply(x1, x2)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 20\n",
    "lr = 0.1\n",
    "image_size = 100\n",
    "batch_size = 16\n",
    "filter_len = 5\n",
    "num_out_ch = 3\n",
    "mp_len = 12\n",
    "fc_nodes = 2\n",
    "W1_len = int((image_size-filter_len+1)/mp_len)\n",
    "decay = 0.8\n",
    "# Declare weights\n",
    "# YOUR CODE HERE\n",
    "# W0 = np.random.normal(0, 0.05, weights['W0'].shape)\n",
    "# W1 = np.random.normal(0, 0.05, weights['W1'].shape)\n",
    "# W2 = np.random.normal(0, 0.05, weights['W2'].shape)\n",
    "# print(W0.shape)\n",
    "# print(W1.shape)\n",
    "# print(W2.shape)\n",
    "W0 = np.random.normal(0, 0.05, (num_out_ch, filter_len, filter_len, 3))\n",
    "W1 = np.random.normal(0, 0.05, (W1_len*W1_len*num_out_ch, fc_nodes))\n",
    "W2 = np.random.normal(0, 0.05, (fc_nodes, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(3, 5, 5, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "i = tf.reshape(W0, [-1, 5, 5, 3])\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'W0_8:0' shape=(3, 5, 5, 3) dtype=float64_ref> <tf.Variable 'W1_2:0' shape=(192, 2) dtype=float64_ref> <tf.Variable 'W2_2:0' shape=(2, 1) dtype=float64_ref>\n"
     ]
    }
   ],
   "source": [
    "W0 = tf.Variable(W0, name='W0')\n",
    "W1 = tf.Variable(W1, name='W1')\n",
    "W2 = tf.Variable(W2, name='W2')\n",
    "print(W0, W1, W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(N, input_image, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(input_image, [N, 100, 100, 3])\n",
    "    \n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=3,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"valid\",\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=12, strides=12)\n",
    "\n",
    "    pool1_flat = tf.reshape(pool1, [N, -1])\n",
    "\n",
    "    # Dense Layer\n",
    "    dense1 = tf.layers.dense(inputs=pool1_flat, units=2, activation=tf.nn.relu)\n",
    "    \n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dense1, units=1)\n",
    "    \n",
    "    sig = tf.layers.\n",
    "\n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "        # `logging_hook`.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Conv2D, MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_keras(x_train, y_train, batch_size, epochs, x_test, y_test):\n",
    "    # several steps\n",
    "    # 1. construct the model\n",
    "    # 2. compile the model\n",
    "    # 3. train the model with the input data\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(3, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding=\"valid\"))\n",
    "    model.add(MaxPooling2D(pool_size=(12, 12), strides=(12, 12)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.SGD(lr=0.1),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks=[history])\n",
    "    \n",
    "    return model\n",
    "\n",
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "        self.loss = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.loss.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 800 samples\n",
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 13s 6ms/step - loss: 0.6105 - acc: 0.6650 - val_loss: 0.4212 - val_acc: 0.8287\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.4006 - acc: 0.8325 - val_loss: 0.3086 - val_acc: 0.8900\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.2780 - acc: 0.8865 - val_loss: 0.2594 - val_acc: 0.8975\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.2778 - acc: 0.8940 - val_loss: 0.3089 - val_acc: 0.8738\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.2332 - acc: 0.9105 - val_loss: 0.2555 - val_acc: 0.9187\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.2117 - acc: 0.9165 - val_loss: 0.2164 - val_acc: 0.9150\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.2110 - acc: 0.9145 - val_loss: 0.2098 - val_acc: 0.9163\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1856 - acc: 0.9275 - val_loss: 0.2647 - val_acc: 0.8838\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1818 - acc: 0.9260 - val_loss: 0.2112 - val_acc: 0.9125\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1693 - acc: 0.9365 - val_loss: 0.2108 - val_acc: 0.9213\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1638 - acc: 0.9335 - val_loss: 0.2095 - val_acc: 0.9263\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1562 - acc: 0.9375 - val_loss: 0.2092 - val_acc: 0.9125\n",
      "Epoch 13/20\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1435 - acc: 0.9435 - val_loss: 0.1862 - val_acc: 0.9287\n",
      "Epoch 14/20\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1399 - acc: 0.9470 - val_loss: 0.2082 - val_acc: 0.9213\n",
      "Epoch 15/20\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.1342 - acc: 0.9470 - val_loss: 0.2070 - val_acc: 0.9237\n",
      "Epoch 16/20\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1188 - acc: 0.9560 - val_loss: 0.2334 - val_acc: 0.9175\n",
      "Epoch 17/20\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1183 - acc: 0.9525 - val_loss: 0.2636 - val_acc: 0.9137\n",
      "Epoch 18/20\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1080 - acc: 0.9565 - val_loss: 0.2339 - val_acc: 0.9113\n",
      "Epoch 19/20\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.1079 - acc: 0.9650 - val_loss: 0.2140 - val_acc: 0.9213\n",
      "Epoch 20/20\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.0927 - acc: 0.9660 - val_loss: 0.3007 - val_acc: 0.9050\n"
     ]
    }
   ],
   "source": [
    "history = AccuracyHistory()\n",
    "model = cnn_keras(train_set[0], train_set[1], 16, 20, val_set[0], val_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1)\n"
     ]
    }
   ],
   "source": [
    "plt.plot(range(1,21), history.acc)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,21), history.loss)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
