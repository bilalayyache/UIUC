{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part is to extract features from input .fea files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "people = ['dg', 'ls', 'yx', 'mh']\n",
    "words = ['asr', 'cnn', 'dnn', 'hmm', 'tts']\n",
    "count = 0\n",
    "feature_dict = dict()\n",
    "for person in people:\n",
    "    temp_dict = dict()\n",
    "    for word in words:\n",
    "        # find the name and word of the file\n",
    "        name_word = person + '_' + word\n",
    "        temp_feature = []\n",
    "        for i in range(1, 6):\n",
    "            index = str(i)\n",
    "            filename = 'LAB2/feature/' + person + '/' + name_word + index + '.fea'\n",
    "            # file = glob.glob(filename)\n",
    "            with open(filename, newline='') as csvfile:\n",
    "                data = np.array(list(csv.reader(csvfile)))\n",
    "            data = data.astype(float)\n",
    "            temp_feature.append(data)\n",
    "            count += 1\n",
    "        temp_dict[word] = temp_feature\n",
    "    feature_dict[person] = temp_dict\n",
    "print(count)\n",
    "    \n",
    "#         filename = 'LAB2/feature/' + person + '/' + name_word + '*.fea'\n",
    "#         files = glob.glob(filename)\n",
    "#         temp_feature = []\n",
    "#         for file in files:\n",
    "#             with open(file, newline='') as csvfile:\n",
    "#                 data = np.array(list(csv.reader(csvfile)))\n",
    "#             data = data.astype(float)\n",
    "#             temp_feature.append(data)\n",
    "#         temp_dict[word] = temp_feature\n",
    "#     feature_dict[person] = temp_dict\n",
    "\n",
    "# dg_tts_files = glob.glob('LAB2/feature/dg/dg_tts*.fea')\n",
    "# dg_tts_feature = []\n",
    "# for filename in dg_tts_files:\n",
    "#     with open(filename, newline='') as csvfile:\n",
    "#         data = np.array(list(csv.reader(csvfile)))\n",
    "#     data = data.astype(float)\n",
    "#     dg_tts_feature.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we get all 100 feature files, we need to separate them into training set and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# people dependent\n",
    "#######################\n",
    "pd_dict = {}\n",
    "pd_dict['train'] = {}\n",
    "pd_dict['test'] = {}\n",
    "# initialize dict\n",
    "for word in words:\n",
    "    pd_dict['train'][word] = []\n",
    "    pd_dict['test'][word] = []\n",
    "\n",
    "# for each word in training set, we have a list contains 15 files\n",
    "# for test set, we have 5 files per word\n",
    "for word in words:\n",
    "    for person in people:\n",
    "        # if the persion is mh, we need to put the feature into the test set\n",
    "        if person == 'mh':\n",
    "            pd_dict['test'][word] += feature_dict[person][word]\n",
    "        # if the person is not mh, we need to put the feature into the train set\n",
    "        else:\n",
    "            pd_dict['train'][word] += feature_dict[person][word]\n",
    "\n",
    "########################\n",
    "# people independent\n",
    "########################\n",
    "pid_dict = {}\n",
    "pid_dict['train'] = {}\n",
    "pid_dict['test'] = {}\n",
    "# initialize dict\n",
    "for word in words:\n",
    "    pid_dict['train'][word] = []\n",
    "    pid_dict['test'][word] = []\n",
    "\n",
    "# for each word in training set, we have a list contains 16 files\n",
    "# for test set, we have 4 files per word\n",
    "for word in words:\n",
    "    for person in people:\n",
    "        # only the first four utterances for each word go to train set\n",
    "        pid_dict['train'][word] += feature_dict[person][word][0:4]\n",
    "        pid_dict['test'][word] += feature_dict[person][word][4:5]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will find the observation likelihood based for Gaussian variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_likelihood(x, mu, i, Sigma):\n",
    "    # x: 1D array, mu: 2D array, i: int (state number), Sigma: 3D array\n",
    "    # this function will return the probability that given state = i, the obs is x\n",
    "    # P(x | q = i)\n",
    "    temp_mu = mu[i:i+1, :]\n",
    "    temp_Sigma = Sigma[i, :, :] * 2 * np.pi\n",
    "    exponent = -0.5 * (x-temp_mu) @ la.inv(Sigma[i,:,:]) @ (x-temp_mu).T\n",
    "    print(exponent)\n",
    "    print(la.det(temp_Sigma)**0.5)\n",
    "    print(temp_Sigma.shape)\n",
    "    prob = np.exp(exponent) / la.det(temp_Sigma)**0.5\n",
    "    # print(exponent)\n",
    "    return prob[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.9608e+00 -1.4349e+01  1.3764e+00 -1.2844e-01  4.6790e-01  3.0451e-01\n",
      "   4.7751e-01  3.7611e-01  4.1144e-01  6.5780e-02  1.4007e-01 -3.8609e-02\n",
      "   2.4849e-03 -1.0674e-01]\n",
      " [-2.7758e+00 -1.4045e+01  1.5075e+00 -3.2848e-01  4.4868e-01  4.8089e-01\n",
      "   6.3896e-01  4.6756e-01  1.1689e-01  4.3011e-02  3.9960e-02  5.2888e-02\n",
      "   2.5877e-02 -2.8850e-02]\n",
      " [-2.6678e+00 -1.3880e+01  1.4971e+00 -6.4713e-01  2.6214e-01  4.7858e-01\n",
      "   5.8093e-01  5.1797e-01  3.5223e-01  2.1355e-02 -2.2420e-01 -1.5485e-01\n",
      "   2.7081e-01 -1.3986e-02]\n",
      " [-2.5468e+00 -1.4132e+01  1.6420e+00 -2.4413e-01  4.7802e-01  4.6996e-01\n",
      "   5.8100e-01  4.7325e-01  1.7617e-01  1.3759e-01  1.1008e-02  9.6269e-03\n",
      "   1.0837e-02  2.2199e-02]\n",
      " [-2.5840e+00 -1.3894e+01  1.7188e+00 -3.0315e-01  4.2633e-01  2.8755e-01\n",
      "   4.6207e-01  6.1846e-01  1.3587e-01 -7.8690e-02  1.0549e-01 -1.8269e-02\n",
      "  -1.7345e-01 -2.3138e-01]\n",
      " [-2.6174e+00 -1.4235e+01  1.5211e+00 -4.1770e-01  3.8450e-01  3.9515e-01\n",
      "   7.0013e-01  6.1862e-01  1.6773e-01  3.3597e-01 -1.3929e-01 -2.7726e-01\n",
      "  -1.5725e-01 -1.1405e-01]\n",
      " [-2.8527e+00 -1.4221e+01  1.5938e+00 -5.1154e-01  2.3481e-01  3.1780e-01\n",
      "   6.1260e-01  5.9183e-01  1.4298e-01 -1.0587e-01 -3.2697e-01 -4.7900e-01\n",
      "  -2.1559e-01  2.2137e-01]\n",
      " [-2.6140e+00 -1.4252e+01  1.4356e+00 -5.7366e-01  2.5042e-01  3.7859e-01\n",
      "   4.7601e-01  7.9448e-01  3.8573e-01  2.1180e-02  1.0996e-01 -3.6293e-01\n",
      "  -3.8252e-02  1.8494e-01]\n",
      " [-2.6125e+00 -1.4080e+01  1.7240e+00 -2.7299e-01  2.5830e-01  3.2895e-01\n",
      "   5.1864e-01  5.7807e-01  2.0177e-01  1.4908e-01  1.6436e-01 -2.6606e-02\n",
      "   1.3446e-01  9.4915e-02]\n",
      " [-2.5333e+00 -1.3754e+01  1.2393e+00 -4.6756e-01  1.7532e-01  3.8407e-01\n",
      "   2.0466e-01  6.8050e-01  3.8920e-01 -2.5078e-02 -2.8064e-02  7.4095e-02\n",
      "  -3.5849e-01 -8.2853e-02]]\n"
     ]
    }
   ],
   "source": [
    "model = My_HMM(pd_dict['train']['asr'])\n",
    "mean = model.get_mu()[0]\n",
    "sigma = model.get_Sigma()[0]\n",
    "print(mean)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class\n",
    "# input_data must be a \n",
    "import numpy as np\n",
    "class My_HMM:\n",
    "    def __init__(self, input_data, N=5):\n",
    "        self.data = input_data\n",
    "        # first, we need to initlaize the class\n",
    "        # in the mp, we have 4 parameters needed to be trained\n",
    "        # pi: an array of size N contains the probability that the first state is i\n",
    "        # A: transition matrix size N * N\n",
    "        # mu: a matrix of size N * D, mean of the Gaussian model for each state\n",
    "        # Sigma: a matrix of size N * D * D, covariance of the Gaussian model for each state\n",
    "        self.N = N\n",
    "        _, self.D = input_data[0].shape\n",
    "        self.pi = np.ones((N, )) / N\n",
    "        self.A = np.eye(N) * 0.8 + np.eye(N, k=1) * 0.2\n",
    "        self.A[N-1, N-1] = 1.0\n",
    "        \n",
    "        # find mu and Sigma\n",
    "        # input data is a list contains 15/16 files, we need to find mean and covariance of them all\n",
    "        mu = 0.0\n",
    "        temp = 0\n",
    "        for data in input_data:\n",
    "            temp_mu = data.mean(0) * data.shape[0]\n",
    "            temp += data.shape[0]\n",
    "            mu += temp_mu\n",
    "        mu /= temp\n",
    "        first_feature = input_data[0]\n",
    "#         m, n = first_feature.shape\n",
    "#         temp_mu = np.sum(first_feature, axis=0) / m\n",
    "        self.mu = np.tile(mu, (N,1))\n",
    "        temp_Sigma = np.cov(first_feature, rowvar=False)\n",
    "        self.Sigma = np.tile(temp_Sigma, (N, 1, 1))\n",
    "        \n",
    "    def get_pi(self):\n",
    "        return self.pi\n",
    "\n",
    "    def get_A(self):\n",
    "        return self.A\n",
    "\n",
    "    def get_mu(self):\n",
    "        return self.mu\n",
    "    \n",
    "    def get_Sigma(self):\n",
    "        return self.Sigma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_asr = feature_dict['mh']['asr']\n",
    "model = My_HMM(dg_asr)\n",
    "# print(model.get_pi())\n",
    "# print(model.get_A())\n",
    "# print(model.get_mu())\n",
    "# print(model.get_Sigma())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = 10\n",
    "a /= 2*5\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
