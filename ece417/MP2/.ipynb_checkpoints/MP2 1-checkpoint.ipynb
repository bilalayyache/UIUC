{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part is to extract features from input .fea files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# In this part, we read the feature from files and store the feature into arrays\n",
    "# In this example, we extract the word 'tts' spoken by speaker dg and store the feature into list named dg_tts_feature\n",
    "\n",
    "# Note of list dg_tts_feature:\n",
    "# Since we have each word spoken 5 times per person, the list contains 5 elements\n",
    "# Each element represents a matrix of size T * D\n",
    "# T is the total number of frames (this may vary in different utterence), D is the dimension of the mfcc (14)\n",
    "people = ['dg', 'ls', 'yx', 'mh']\n",
    "words = ['asr', 'cnn', 'dnn', 'hmm', 'tts']\n",
    "count = 0\n",
    "feature_dict = dict()\n",
    "for person in people:\n",
    "    temp_dict = dict()\n",
    "    for word in words:\n",
    "        # find the name and word of the file\n",
    "        name_word = person + '_' + word\n",
    "        temp_feature = []\n",
    "        for i in range(1, 6):\n",
    "            index = str(i)\n",
    "            filename = 'LAB2/feature/' + person + '/' + name_word + index + '.fea'\n",
    "            # file = glob.glob(filename)\n",
    "            with open(filename, newline='') as csvfile:\n",
    "                data = np.array(list(csv.reader(csvfile)))\n",
    "            data = data.astype(float)\n",
    "            temp_feature.append(data)\n",
    "            count += 1\n",
    "        temp_dict[word] = temp_feature\n",
    "    feature_dict[person] = temp_dict\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we get all 100 feature files, we need to separate them into training set and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# people dependent\n",
    "#######################\n",
    "pd_dict = {}\n",
    "pd_dict['train'] = {}\n",
    "pd_dict['test'] = {}\n",
    "# initialize dict\n",
    "for word in words:\n",
    "    pd_dict['train'][word] = []\n",
    "    pd_dict['test'][word] = []\n",
    "\n",
    "# for each word in training set, we have a list contains 15 files\n",
    "# for test set, we have 5 files per word\n",
    "for word in words:\n",
    "    for person in people:\n",
    "        # if the persion is mh, we need to put the feature into the test set\n",
    "        if person == 'mh':\n",
    "            pd_dict['test'][word] += feature_dict[person][word]\n",
    "        # if the person is not mh, we need to put the feature into the train set\n",
    "        else:\n",
    "            pd_dict['train'][word] += feature_dict[person][word]\n",
    "\n",
    "########################\n",
    "# people independent\n",
    "########################\n",
    "pid_dict = {}\n",
    "pid_dict['train'] = {}\n",
    "pid_dict['test'] = {}\n",
    "# initialize dict\n",
    "for word in words:\n",
    "    pid_dict['train'][word] = []\n",
    "    pid_dict['test'][word] = []\n",
    "\n",
    "# for each word in training set, we have a list contains 16 files\n",
    "# for test set, we have 4 files per word\n",
    "for word in words:\n",
    "    for person in people:\n",
    "        # only the first four utterances for each word go to train set\n",
    "        pid_dict['train'][word] += feature_dict[person][word][0:4]\n",
    "        pid_dict['test'][word] += feature_dict[person][word][4:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will find the observation likelihood based for Gaussian variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_likelihood(x, mu, i, Sigma):\n",
    "    # x: 1D array, mu: 2D array, i: int (state number), Sigma: 3D array\n",
    "    # this function will return the probability that given state = i, the obs is x\n",
    "    # P(x | q = i)\n",
    "    temp_mu = mu[i:i+1, :]\n",
    "    temp_Sigma = Sigma[i, :, :] * 2 * np.pi\n",
    "    exponent = -0.5 * (x-temp_mu) @ la.inv(Sigma[i,:,:])@(x-temp_mu).T\n",
    "    prob = np.exp(exponent) / la.det(temp_Sigma)**0.5\n",
    "    # print(exponent)\n",
    "    return prob[0, 0]\n",
    "\n",
    "# from scipy.stats import multivariate_normal as mvn\n",
    "# def emision(X, means, cov):\n",
    "#     e_p = np.zeros((X.shape[0], 5))\n",
    "#     for t in range(X.shape[0]):\n",
    "#         for k in range(5):\n",
    "#             e_p[t, k] = mvn.pdf(X[t, :], means[k], cov[k], allow_singular=True)\n",
    "#     return e_p\n",
    "\n",
    "# model_cnn = My_HMM(cnn_train_data)\n",
    "# mean = model_cnn.get_mu()\n",
    "# sigma = model_cnn.get_Sigma()\n",
    "# means = mean[0, :]\n",
    "# cov = sigma[0, :, :]\n",
    "# print('mean is', means)\n",
    "# print('cov is', cov)\n",
    "# p_1 = get_obs_likelihood(cnn_train_data[0][0, :], mean, 0, sigma)\n",
    "# p_2 = emision(cnn_train_data[0], means, cov)\n",
    "# print(p_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-daa459a94f72>, line 51)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-daa459a94f72>\"\u001b[0;36m, line \u001b[0;32m51\u001b[0m\n\u001b[0;31m    def mixture_gaussian(self, X, mu, Sigma, c)\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# define a class\n",
    "# input_data must be a \n",
    "import numpy as np\n",
    "class My_HMM:\n",
    "    def __init__(self, train_data, N=5):\n",
    "        self.train_data = train_data\n",
    "        # self.test_data = test_data\n",
    "        self.train_data_matrix = np.vstack(train_data)\n",
    "        # first, we need to initlaize the class\n",
    "        # in the mp, we have 4 parameters needed to be trained\n",
    "        # pi: an array of size N contains the probability that the first state is i\n",
    "        # A: transition matrix size N * N\n",
    "        # mu: a matrix of size N * D, mean of the Gaussian model for each state\n",
    "        # Sigma: a matrix of size N * D * D, covariance of the Gaussian model for each state\n",
    "        self.N = N-1\n",
    "        _, self.D = train_data[0].shape\n",
    "        self.pi = np.ones((N, )) / N\n",
    "        self.A = np.eye(N) * 0.8 + np.eye(N, k=1) * 0.2\n",
    "        self.A[N-1, N-1] = 1.0\n",
    "        \n",
    "        # find mu and Sigma\n",
    "        # temporarily, we use the mean and covariance of the first file in input data\n",
    "        # first_feature = input_data[0]\n",
    "        m, n = self.train_data_matrix.shape\n",
    "        temp_mu = self.train_data_matrix.mean(0)\n",
    "        self.mu = np.tile(temp_mu, (N,1))\n",
    "        temp_Sigma = np.cov(self.train_data_matrix, rowvar=False)\n",
    "        self.Sigma = np.tile(temp_Sigma, (N, 1, 1))\n",
    "        # this is for Gaussian Mixture Model\n",
    "        self.c = np.ones((N, 3))/3\n",
    "        \n",
    "    def get_pi(self):\n",
    "        return self.pi\n",
    "\n",
    "    def get_A(self):\n",
    "        return self.A\n",
    "\n",
    "    def get_mu(self):\n",
    "        return self.mu\n",
    "    \n",
    "    def get_Sigma(self):\n",
    "        return self.Sigma\n",
    "    \n",
    "    def likelihood(self, X, mu, Sigma):\n",
    "        T, _ = X.shape\n",
    "        B = np.zeros((T, self.N))\n",
    "        for t in range(T):\n",
    "            for i in range(self.N):\n",
    "                B[t, i]=stats.multivariate_normal(mu[i], Sigma[i]).pdf(X[t])\n",
    "        return B\n",
    "    \n",
    "    def mixture_gaussian(self, X, mu, Sigma, c)\n",
    "        T,_ = X.shape\n",
    "        B = np.zeros((3,self.N, T))\n",
    "        for t in range (T):\n",
    "            for i in range (self.N):\n",
    "                B[0][t, i]=stats.multivariate_normal(mu[0][i], Sigma[0][i]).pdf(X[t])\n",
    "                B[1][t, i]=stats.multivariate_normal(mu[1][i], Sigma[1][i]).pdf(X[t])\n",
    "                B[2][t, i]=stats.multivariate_normal(mu[2][i], Sigma[2][i]).pdf(X[t])\n",
    "        \n",
    "        prob = c[:,0].reshape(-1, 1)*B[0] + c[:,1].reshape(-1, 1)*B[1] + c[:,2].reshape(-1, 1)*B[2]\n",
    "        return prob, B\n",
    "\n",
    "    def forward(self, X, pi, A, mu, Sigma, B):\n",
    "        # in this method, we will construct the forward algorithm\n",
    "        # we will use the dynamic programming to store the alpha array while computing the value\n",
    "        T, _ = X.shape\n",
    "        alpha = np.zeros((T, self.N))\n",
    "        g = np.zeros((T, ))\n",
    "        # first, we need to initialize alpha at time 0\n",
    "        for i in range(self.N):\n",
    "            alpha[0, i] = pi[i] * B[0, i]\n",
    "             #alpha[0, i] = pi[i] * get_obs_likelihood(X[0:1, :], mu, i, Sigma)\n",
    "        # normalize\n",
    "        g[0] = np.sum(alpha[0, :])\n",
    "        alpha[0:1, :] = alpha[0:1, :] / g[0]\n",
    "        # iteration\n",
    "        for t in range(1, T):\n",
    "            # for each frame\n",
    "            for j in range(self.N):\n",
    "                # for each state\n",
    "                for i in range(self.N):\n",
    "                    alpha[t, j] += alpha[t-1, i]*A[i, j]*B[t, j]\n",
    "                    # alpha[t, j] += alpha[t-1, i]*A[i, j]*get_obs_likelihood(X[t:t+1, :], mu, j, Sigma)\n",
    "            # normalize\n",
    "            g[t] = np.sum(alpha[t, :])\n",
    "            alpha[t:t+1, :] = alpha[t:t+1, :] / g[t]\n",
    "        # return alpha and g\n",
    "        return alpha, g\n",
    "    \n",
    "    def backward(self, X, pi, A, mu, Sigma, B, g):\n",
    "        T, _ = X.shape\n",
    "#         beta = np.zeros((T, self.N))\n",
    "#         # we use the same normalize factor as we used in forward algorithm\n",
    "#         for i in range(self.N):\n",
    "#             beta[T-1, i] = 1.0\n",
    "#         # iteration\n",
    "#         for t in range(T-2, -1, -1):\n",
    "#             print('method 1, t =', t)\n",
    "#             # for each frame\n",
    "#             for i in range(self.N):\n",
    "#                 # for each state\n",
    "#                 for j in range(self.N):\n",
    "#                     beta[t, i] += beta[t+1, j]*A[i, j]*B[t+1, j]\n",
    "#                     # beta[t, i] += beta[t+1, j]*A[i, j]*get_obs_likelihood(X[t+1:t+2, :], mu, j, Sigma)\n",
    "#             # normalize\n",
    "#             beta[t:t+1, :] = beta[t:t+1, :] / g[t+1]\n",
    "#             print('beta =', beta[t, :])\n",
    "#         return beta\n",
    "\n",
    "        beta = np.zeros((T, self.N))\n",
    "        for i in range(self.N):\n",
    "            beta[T-1, i] = 1.0\n",
    "        for t in range(T-2, -1, -1):\n",
    "            obs = B[t+1, :]\n",
    "            # obs = [get_obs_likelihood(X[t+1:t+2, :], mu, j, Sigma) for j in range(self.N)]\n",
    "            temp_obs = np.diag(obs)\n",
    "            beta[t:t+1, :] = beta[t+1:t+2, :] @ temp_obs @ A.T / g[t+1]\n",
    "        return beta\n",
    "    \n",
    "    def for_back(self, X, pi, A, mu, Sigma, B):\n",
    "        T, _ = X.shape\n",
    "        alpha, g = self.forward(X, pi, A, mu, Sigma, B)\n",
    "        beta = self.backward(X, pi, A, mu, Sigma, B, g)\n",
    "        gamma = np.zeros((T, self.N))\n",
    "        for t in range(T):\n",
    "            gamma[t:t+1, :] = alpha[t:t+1, :]*beta[t:t+1, :]\n",
    "            gamma[t:t+1, :] = gamma[t:t+1, :] / np.sum(gamma[t:t+1, :])\n",
    "        return alpha, beta, gamma \n",
    "    \n",
    "    \n",
    "    def find_zai(self, X, pi, A, mu, Sigma, alpha, beta, B):\n",
    "        T, _ = X.shape\n",
    "        zai = np.zeros((T, 2*self.N))\n",
    "        for t in range(T):\n",
    "            for i in range(self.N):\n",
    "                for j in range(i, i+2):\n",
    "                    zai[t,i+j] = \n",
    "                    if j == self.N:\n",
    "                        zai[t, i+j] = 0.0\n",
    "                    elif(t == T-1):\n",
    "                        zai[t, i+j] = 0.0\n",
    "                    # special case\n",
    "                    else:\n",
    "                        # b = get_obs_likelihood(X[t+1:t+2, :], mu, j, Sigma)\n",
    "                        zai[t, i+j] = alpha[t, i]*A[i, j]*B[t+1, j]*beta[t+1, j]\n",
    "            # now we normalize zai_t(i, j)\n",
    "            temp_sum = np.sum(zai[t, :])\n",
    "            zai[t:t+1, :] /= temp_sum\n",
    "        return zai\n",
    "    \n",
    "    def training(self):\n",
    "        # in this file, we will go through all files and get all alpha beta value\n",
    "        alpha_list = []\n",
    "        beta_list = []\n",
    "        gamma_list = []\n",
    "        zai_list = []\n",
    "        for data in self.train_data:\n",
    "            B = self.likelihood(data, self.mu, self.Sigma)\n",
    "            temp_alpha, temp_beta, temp_gamma = self.for_back(data, self.pi, self.A, self.mu, self.Sigma, B)\n",
    "            temp_zai = self.find_zai(data, self.pi, self.A, self.mu, self.Sigma, temp_alpha, temp_beta, B)\n",
    "            alpha_list.append(temp_alpha)\n",
    "            beta_list.append(temp_beta)\n",
    "            gamma_list.append(temp_gamma)\n",
    "            zai_list.append(temp_zai)\n",
    "        return alpha_list, beta_list, gamma_list, zai_list\n",
    "    \n",
    "    def update(self):\n",
    "        # in this function, we will use the F/B algorithm to find alpha and beta first\n",
    "        # and use alpha and beta to update self.mu, self.Sigma and self.A\n",
    "        alpha_list, beta_list, gamma_list, zai_list = self.training()\n",
    "        new_A = np.zeros_like(self.A)\n",
    "        new_mu = np.zeros_like(self.mu)\n",
    "        new_Sigma = np.zeros_like(self.Sigma)\n",
    "        # first, we can sum the gamma value\n",
    "        sum_gamma = np.zeros((self.N, ))\n",
    "        for i in range(self.N):\n",
    "            temp_sum = 0\n",
    "            for l in range(len(gamma_list)):\n",
    "                for t in range(gamma_list[l].shape[0]):\n",
    "                    temp_sum += gamma_list[l][t, i]\n",
    "            sum_gamma[i] = temp_sum\n",
    "                    \n",
    "        # now we update A\n",
    "        for i in range(self.N):\n",
    "            for j in range(i, i+2):\n",
    "                if j < self.N:\n",
    "                    if (j == i+1) or (j == i and j == self.N):\n",
    "                        new_A[i, j] = 1 - new_A[i, j-1]\n",
    "                    else:\n",
    "                        for l in range(len(self.train_data)):\n",
    "                            for t in range(self.train_data[l].shape[0]):\n",
    "                                new_A[i, j] += np.sum(zai_list[l][t, i+j])\n",
    "            new_A[i, :] /= sum_gamma[i]\n",
    "            \n",
    "        # now we update mu\n",
    "        for i in range(self.N):\n",
    "            temp_mu = np.zeros((1, self.D))\n",
    "            for l in range(len(self.train_data)):\n",
    "                for t in range(self.train_data[l].shape[0]):\n",
    "                    #temp_mu += self.train_data[l][t:t+1, :]* gamma_list[l][t, i]\n",
    "                    temp_mu = temp_mu + np.inner(gamma_list[l][t, i],self.train_data[l][t:t+1, :])\n",
    "            temp_mu /= sum_gamma[i]\n",
    "            new_mu[i:i+1, :] = temp_mu\n",
    "        \n",
    "        # new we update Sigma\n",
    "        for i in range(self.N):\n",
    "            temp_Sigma = np.zeros((self.D, self.D))\n",
    "            for l in range(len(self.train_data)):\n",
    "                for t in range(self.train_data[l].shape[0]):\n",
    "                    #temp = self.train_data[l][t:t+1, :] - new_mu[i:i+1, :]\n",
    "                    #temp_Sigma += (temp.T @ temp) * gamma_list[l][t, i]\n",
    "                    temp = self.train_data[l][t:t+1, :] - new_mu[i:i+1, :]\n",
    "                    temp_Sigma += gamma_list[l][t, i]*np.outer(temp, temp);\n",
    "            temp_Sigma /= sum_gamma[i]\n",
    "            new_Sigma[i, :, :] = temp_Sigma\n",
    "        \n",
    "        # update here\n",
    "        self.A = new_A\n",
    "        self.mu = new_mu\n",
    "        self.Sigma = new_Sigma\n",
    "\n",
    "    def new_update(self)\n",
    "        alpha_list, beta_list, gamma_list, zai_list = self.training()\n",
    "        new_A = np.zeros_like(self.A)\n",
    "        new_mu = np.zeros_like(self.mu)\n",
    "        new_Sigma = np.zeros_like(self.Sigma)\n",
    "        sum_gamma = np.zeros((self.N, ))\n",
    "        m_sum_gamma = np.zeros((self.N,3))\n",
    "        # sum_gamma for A\n",
    "        for i in range(self.N):\n",
    "            temp_sum = 0\n",
    "            for l in range(len(gamma_list)):\n",
    "                for t in range(gamma_list[l].shape[0]):\n",
    "                    temp_sum += gamma_list[l][t, i]\n",
    "            sum_gamma[i] = temp_sum\n",
    "        # update new A\n",
    "        for i in range(self.N):\n",
    "            for j in range(i, i+2):\n",
    "                if j < self.N:\n",
    "                    if (j == i+1) or (j == i and j == self.N):\n",
    "                        new_A[i, j] = 1 - new_A[i, j-1]\n",
    "                    else:\n",
    "                        for l in range(len(self.train_data)):\n",
    "                            for t in range(self.train_data[l].shape[0]):\n",
    "                                new_A[i, j] += np.sum(zai_list[l][t, i+j])\n",
    "            new_A[i, :] /= sum_gamma[i]\n",
    "            \n",
    "        #update new_sum_gama\n",
    "        for l in range(len(self.train_data)):\n",
    "            for m in range(3):\n",
    "                new_sum_gamma[l][m] = np.zeros(gamma[l].shape)\n",
    "                for i in range(self.N):\n",
    "                    new_sum_gamma[l][i,:] = gamma[l][i, :]*c[i][m]*B[l][m][i, :]/(B[l][0][i,:]+B[l][1][i,:]+B[l][2][i,:])\n",
    "        #update the m_sum_gamma\n",
    "        for i in range(self.N):\n",
    "            temp_sum = 0\n",
    "            for m in range(3):\n",
    "                for l in range(len(self.train_data)):\n",
    "                    m_sum_gamma[i,m] += np.sum(new_sum_gamma[l][m][i,:])\n",
    "        \n",
    "        #updata c\n",
    "        for i in range(self.N):\n",
    "            for m in range(3):\n",
    "                new_c[i][m] = 0\n",
    "                for l in range(len(self.train_data)):\n",
    "                    new_c[i][m]+=np.sum(new_sum_gamma[l][m][i,:])\n",
    "                new_c[i][m] /= m_sum_gamma[i]\n",
    "        # update mu\n",
    "        for m in range(3):\n",
    "            for i in range(self.N):\n",
    "                mu[m][i] = 0\n",
    "                for l in range(len(self.train_data)):\n",
    "                    mu[m][i] += np.inner(self.train_data[l][t:t+1, :],new_sum_gamma[l][m][i,:])\n",
    "            mu[m][i] /= m_sum_gamma[i,m]\n",
    "        # update sigma\n",
    "        for i in range(self.N)\n",
    "            for m in range(3)\n",
    "                new_Sigma[m][i] = 0.0*np.identity(14)\n",
    "                for l in range(len(self.train_data)):\n",
    "                    for t in range(self.train_data[l].shape[0]):\n",
    "                        temp = self.train_data[l][t:t+1, :] - mu[m][i]\n",
    "                        new_Sigma[m][i] += new_sum_gamma[l][m][i,t]*np.outer(temp, temp)/m_sum_gamma[i,m]\n",
    "                    \n",
    "        \n",
    "        self.A = new_A\n",
    "        self.mu = new_mu\n",
    "        self.Sigma = new_Sigma\n",
    "        self.c = new_c\n",
    "        \n",
    "#     def training(self, data):\n",
    "#         B = self.likelihood(data, self.mu, self.Sigma)\n",
    "#         alpha, beta, gamma = self.for_back(data, self.pi, self.A, self.mu, self.Sigma, B)\n",
    "#         zai = self.find_zai(data, self.pi, self.A, self.mu, self.Sigma, alpha, beta, B)\n",
    "#         return alpha, beta, gamma, zai\n",
    "    \n",
    "#     def update(self, data):\n",
    "#         alpha, beta, gamma, zai = self.training(data)\n",
    "#         new_A = np.zeros_like(self.A)\n",
    "#         new_mu = np.zeros_like(self.mu)\n",
    "#         new_Sigma = np.zeros_like(self.Sigma)\n",
    "#         # first, we can sum the gamma value\n",
    "#         sum_gamma = np.sum(gamma, axis=0)\n",
    "\n",
    "#         # now we update A\n",
    "#         sum_zai = np.sum(zai, axis=0)\n",
    "#         for i in range(self.N):\n",
    "#             for j in range(i, i+2):\n",
    "#                 if j < self.N:\n",
    "#                     new_A[i, j] = sum_zai[i+j]\n",
    "#             new_A[i, :] /= sum_gamma[i]\n",
    "            \n",
    "#         # now we update mu\n",
    "#         for i in range(self.N):\n",
    "#             temp_mu = np.zeros((1, self.D))\n",
    "#             for t in range(data.shape[0]):\n",
    "#                 temp_mu += data[t:t+1, :] * gamma[t, i]\n",
    "#             temp_mu /= sum_gamma[i]\n",
    "#             new_mu[i:i+1, :] = temp_mu        \n",
    "        \n",
    "#         # new we update Sigma\n",
    "#         for i in range(self.N):\n",
    "#             temp_Sigma = np.zeros((self.D, self.D))\n",
    "#             for t in range(data.shape[0]):\n",
    "#                 temp = data[t:t+1, :] - new_mu[i:i+1, :]\n",
    "#                 temp_Sigma += np.outer(temp, temp) * gamma[t, i]\n",
    "#             temp_Sigma /= sum_gamma[i]\n",
    "#             # deal with singular matrix\n",
    "#             if la.det(temp_Sigma) < 1e-10:\n",
    "#                 temp_Sigma += 0.5*np.eye(self.D)\n",
    "#                 # print(la.det(temp_Sigma))\n",
    "#             new_Sigma[i] = temp_Sigma\n",
    "        \n",
    "#         # update here\n",
    "#         self.A = new_A\n",
    "#         self.mu = new_mu\n",
    "#         self.Sigma = new_Sigma\n",
    "        \n",
    "    def iterate(self):\n",
    "        # in this function, we will iterate through all file\n",
    "        for i in range(10):\n",
    "            self.update()\n",
    "          \n",
    "    def test(self, test_data):\n",
    "        # in this method, we will test on the test input data\n",
    "        retval = np.zeros((len(test_data, )))\n",
    "        for i, data in enumerate(test_data):\n",
    "            T, _ = data.shape\n",
    "            B = self.likelihood(data, self.mu, self.Sigma)\n",
    "            alpha, g = self.forward(data, self.pi, self.A, self.mu, self.Sigma, B)\n",
    "            retval[i] = np.sum(np.log(g[T-1]))\n",
    "        return retval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = ['asr', 'cnn', 'dnn', 'hmm', 'tts']\n",
    "def final_test(words, test_dict, model_asr, model_cnn, model_dnn, model_hmm, model_tts):\n",
    "    all_decisions = []\n",
    "    for word in words:\n",
    "        single_decision = []\n",
    "        test_data = test_dict[word]\n",
    "        temp_decision = np.zeros((5, len(test_data)))\n",
    "        temp_decision[0, :] = model_asr.test(test_data)\n",
    "        temp_decision[1, :] = model_cnn.test(test_data)\n",
    "        temp_decision[2, :] = model_dnn.test(test_data)\n",
    "        temp_decision[3, :] = model_hmm.test(test_data)\n",
    "        temp_decision[4, :] = model_tts.test(test_data)\n",
    "        for i in range(len(test_data)):\n",
    "            single_decision.append(np.argmax(temp_decision[:,i]))\n",
    "        all_decisions.append(single_decision)\n",
    "    return all_decisions\n",
    "\n",
    "def accuracy(decisions):\n",
    "    acc = np.zeros((5, 5))\n",
    "    for i in range(len(decision)):\n",
    "        for j in range(len(decision[i])):\n",
    "            acc[i, decision[i][j]] += 1\n",
    "        acc[i:i+1, :] /= np.sum(acc[i:i+1, :])\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:134: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e14d7c8cd570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel_tts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMy_HMM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtts_train_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel_asr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel_dnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f0a4503c41d7>\u001b[0m in \u001b[0;36miterate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;31m#             for data in self.train_data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;31m#                 self.update(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f0a4503c41d7>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# in this function, we will use the F/B algorithm to find alpha and beta first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# and use alpha and beta to update self.mu, self.Sigma and self.A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0malpha_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzai_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mnew_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mnew_mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f0a4503c41d7>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mzai_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mtemp_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_gamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_back\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mtemp_zai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_zai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f0a4503c41d7>\u001b[0m in \u001b[0;36mlikelihood\u001b[0;34m(self, X, mu, Sigma)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSigma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, mean, cov, allow_singular, seed)\u001b[0m\n\u001b[1;32m    356\u001b[0m         return multivariate_normal_frozen(mean, cov,\n\u001b[1;32m    357\u001b[0m                                           \u001b[0mallow_singular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_singular\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m                                           seed=seed)\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mean, cov, allow_singular, seed, maxpts, abseps, releps)\u001b[0m\n\u001b[1;32m    726\u001b[0m         self.dim, self.mean, self.cov = self._dist._process_parameters(\n\u001b[1;32m    727\u001b[0m                                                             None, mean, cov)\n\u001b[0;32m--> 728\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_PSD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_singular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmaxpts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mmaxpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, M, cond, rcond, lower, check_finite, allow_singular)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Note that eigh takes care of array conversion, chkfinite,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# and assertion that the matrix is square.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_eigvalsh_to_eps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/linalg/decomp.py\u001b[0m in \u001b[0;36meigh\u001b[0;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected square matrix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m_asarray_validated\u001b[0;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'masked arrays are not supported'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AllFloat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         raise ValueError(\n\u001b[0;32m-> 1233\u001b[0;31m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[1;32m   1234\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "cnn_train_data = pd_dict['train']['cnn']\n",
    "dnn_train_data = pd_dict['train']['dnn']\n",
    "asr_train_data = pd_dict['train']['asr']\n",
    "hmm_train_data = pd_dict['train']['hmm']\n",
    "tts_train_data = pd_dict['train']['tts']\n",
    "# train data\n",
    "model_asr = My_HMM(asr_train_data)\n",
    "model_cnn = My_HMM(cnn_train_data)\n",
    "model_dnn = My_HMM(dnn_train_data)\n",
    "model_hmm = My_HMM(hmm_train_data)\n",
    "model_tts = My_HMM(tts_train_data)\n",
    "\n",
    "model_asr.iterate()\n",
    "model_cnn.iterate()\n",
    "model_dnn.iterate()\n",
    "model_hmm.iterate()\n",
    "model_tts.iterate()\n",
    "\n",
    "test_dict = pd_dict['test']\n",
    "decision = final_test(words, test_dict, model_asr, model_cnn, model_dnn, model_hmm, model_tts)\n",
    "print(decision)\n",
    "acc = accuracy(decision)\n",
    "print(acc)\n",
    "# model_dnn = My_HMM(dnn_train_data)\n",
    "# model_dnn.training()\n",
    "# model_asr = My_HMM(asr_train_data)\n",
    "# model_asr.training()\n",
    "# model_hmm = My_HMM(hmm_train_data)\n",
    "# model_hmm.training()\n",
    "# model_tts = My_HMM(tts_train_data)\n",
    "# model_tts.training()\n",
    "# test data\n",
    "# asr_test_data = pd_dict['test']['asr']\n",
    "# cnn_result = model_cnn.test(cnn_test_data)\n",
    "# dnn_result = model_dnn.test(cnn_test_data)\n",
    "# asr_result = model_asr.test(asr_test_data)\n",
    "# hmm_result = model_hmm.test(cnn_test_data)\n",
    "# tts_result = model_tts.test(cnn_test_data)\n",
    "\n",
    "# alpha, g = model_asr.forward(asr_train_data[0], pi, A, mu, Sigma)\n",
    "# beta = model_asr.backward(asr_train_data[0], pi, A, mu, Sigma, g)\n",
    "# print(beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 3, 3, 3, 3], [3, 0, 3, 3, 3], [0, 0, 3, 3, 3], [3, 3, 3, 3, 3], [0, 0, 3, 3, 3]]\n",
      "[[0.  0.  0.  1.  0. ]\n",
      " [0.2 0.  0.  0.8 0. ]\n",
      " [0.4 0.  0.  0.6 0. ]\n",
      " [0.  0.  0.  1.  0. ]\n",
      " [0.4 0.  0.  0.6 0. ]]\n"
     ]
    }
   ],
   "source": [
    "print(decision)\n",
    "acc = accuracy(decision)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  6 11]\n",
      "[[ 1]\n",
      " [ 6]\n",
      " [11]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
