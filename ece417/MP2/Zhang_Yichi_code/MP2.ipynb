{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part is to extract features from input .fea files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# In this part, we read the feature from files and store the feature into arrays\n",
    "# In this example, we extract the word 'tts' spoken by speaker dg and store the feature into list named dg_tts_feature\n",
    "\n",
    "# Note of list dg_tts_feature:\n",
    "# Since we have each word spoken 5 times per person, the list contains 5 elements\n",
    "# Each element represents a matrix of size T * D\n",
    "# T is the total number of frames (this may vary in different utterence), D is the dimension of the mfcc (14)\n",
    "people = ['dg', 'ls', 'yx', 'mh']\n",
    "words = ['asr', 'cnn', 'dnn', 'hmm', 'tts']\n",
    "count = 0\n",
    "feature_dict = dict()\n",
    "for person in people:\n",
    "    temp_dict = dict()\n",
    "    for word in words:\n",
    "        # find the name and word of the file\n",
    "        name_word = person + '_' + word\n",
    "        temp_feature = []\n",
    "        for i in range(1, 6):\n",
    "            index = str(i)\n",
    "            filename = 'LAB2/feature/' + person + '/' + name_word + index + '.fea'\n",
    "            with open(filename, newline='') as csvfile:\n",
    "                data = np.array(list(csv.reader(csvfile)))\n",
    "            data = data.astype(float)\n",
    "            temp_feature.append(data)\n",
    "            count += 1\n",
    "        temp_dict[word] = temp_feature\n",
    "    feature_dict[person] = temp_dict\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "dd_dict = {}\n",
    "count = 0\n",
    "for word in words:\n",
    "    temp_feature = []\n",
    "    for i in range(1, 6):\n",
    "        index = str(i)\n",
    "        filename = 'dd/dd_' + word + '_' + index + '.fea'\n",
    "        with open(filename, newline='') as csvfile:\n",
    "            data = np.array(list(csv.reader(csvfile)))\n",
    "        data = data.astype(float)\n",
    "        temp_feature.append(data)\n",
    "        count += 1\n",
    "    dd_dict[word] = temp_feature\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we get all 100 feature files, we need to separate them into training set and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# people independent\n",
    "#######################\n",
    "pid_dict = {}\n",
    "pid_dict['train'] = {}\n",
    "pid_dict['test'] = {}\n",
    "# initialize dict\n",
    "for word in words:\n",
    "    pid_dict['train'][word] = []\n",
    "    pid_dict['test'][word] = []\n",
    "\n",
    "# for each word in training set, we have a list contains 15 files\n",
    "# for test set, we have 5 files per word\n",
    "for word in words:\n",
    "    for person in people:\n",
    "        # if the persion is mh, we need to put the feature into the test set\n",
    "        if person == 'mh':\n",
    "            pid_dict['test'][word] += feature_dict[person][word]\n",
    "        # if the person is not mh, we need to put the feature into the train set\n",
    "        else:\n",
    "            pid_dict['train'][word] += feature_dict[person][word]\n",
    "\n",
    "########################\n",
    "# people dependent\n",
    "########################\n",
    "pd_dict = {}\n",
    "pd_dict['train'] = {}\n",
    "pd_dict['test'] = {}\n",
    "# initialize dict\n",
    "for word in words:\n",
    "    pd_dict['train'][word] = []\n",
    "    pd_dict['test'][word] = []\n",
    "\n",
    "# for each word in training set, we have a list contains 16 files\n",
    "# for test set, we have 4 files per word\n",
    "for word in words:\n",
    "    for person in people:\n",
    "        # only the first four utterances for each word go to train set\n",
    "        pd_dict['train'][word] += feature_dict[person][word][0:4]\n",
    "        pd_dict['test'][word] += feature_dict[person][word][4:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will find the observation likelihood based for Gaussian variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_likelihood(x, mu, i, Sigma):\n",
    "    # x: 1D array, mu: 2D array, i: int (state number), Sigma: 3D array\n",
    "    # this function will return the probability that given state = i, the obs is x\n",
    "    # P(x | q = i)\n",
    "    temp_mu = mu[i:i+1, :]\n",
    "    temp_Sigma = Sigma[i, :, :] * 2 * np.pi\n",
    "    print(x-temp_mu)\n",
    "    exponent = -0.5 * (x-temp_mu) @ la.inv(Sigma[i,:,:])@(x-temp_mu).T\n",
    "    prob = np.exp(exponent) / la.det(temp_Sigma)**0.5\n",
    "    # print(exponent)\n",
    "    return prob[0, 0]\n",
    "\n",
    "# from scipy.stats import multivariate_normal as mvn\n",
    "# def emision(X, means, cov):\n",
    "#     e_p = np.zeros((X.shape[0], 5))\n",
    "#     for t in range(X.shape[0]):\n",
    "#         for k in range(5):\n",
    "#             e_p[t, k] = mvn.pdf(X[t, :], means[k], cov[k], allow_singular=True)\n",
    "#     return e_p\n",
    "\n",
    "# model_cnn = My_HMM(cnn_train_data)\n",
    "# mean = model_cnn.get_mu()\n",
    "# sigma = model_cnn.get_Sigma()\n",
    "# means = mean[0, :]\n",
    "# cov = sigma[0, :, :]\n",
    "# print('mean is', means)\n",
    "# print('cov is', cov)\n",
    "# p_1 = get_obs_likelihood(cnn_train_data[0][0, :], mean, 0, sigma)\n",
    "# p_2 = emision(cnn_train_data[0], means, cov)\n",
    "# print(p_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class\n",
    "# input_data must be a \n",
    "import numpy as np\n",
    "class My_HMM:\n",
    "    def __init__(self, train_data, N=5, GMM=False):\n",
    "        self.train_data = train_data\n",
    "        # self.test_data = test_data\n",
    "        self.train_data_matrix = np.vstack(train_data)\n",
    "        # first, we need to initlaize the class\n",
    "        # in the mp, we have 4 parameters needed to be trained\n",
    "        # pi: an array of size N contains the probability that the first state is i\n",
    "        # A: transition matrix size N * N\n",
    "        # mu: a matrix of size N * D, mean of the Gaussian model for each state\n",
    "        # Sigma: a matrix of size N * D * D, covariance of the Gaussian model for each state\n",
    "        self.N = N-1\n",
    "        _, self.D = train_data[0].shape\n",
    "        self.pi = np.ones((N, )) / N\n",
    "        self.A = np.eye(N) * 0.8 + np.eye(N, k=1) * 0.2\n",
    "        self.A[N-1, N-1] = 1.0\n",
    "        \n",
    "        # find mu and Sigma\n",
    "        # temporarily, we use the mean and covariance of the first file in input data\n",
    "        # first_feature = input_data[0]\n",
    "        m, n = self.train_data_matrix.shape\n",
    "        temp_mu = self.train_data_matrix.mean(0)\n",
    "        if GMM:\n",
    "            self.mu = np.tile(temp_mu, (3, N, 1))\n",
    "        else:\n",
    "            self.mu = np.tile(temp_mu, (N, 1))\n",
    "        temp_Sigma = np.cov(self.train_data_matrix, rowvar=False)\n",
    "        if GMM:\n",
    "            self.Sigma = np.tile(temp_Sigma, (3, N, 1, 1))\n",
    "        else:\n",
    "            self.Sigma = np.tile(temp_Sigma, (N, 1, 1))\n",
    "        # this is for Gaussian Mixture Model\n",
    "        self.c = np.ones((3, self.N))/3\n",
    "        self.GMM = GMM\n",
    "        \n",
    "    def get_pi(self):\n",
    "        return self.pi\n",
    "\n",
    "    def get_A(self):\n",
    "        return self.A\n",
    "\n",
    "    def get_mu(self):\n",
    "        return self.mu\n",
    "    \n",
    "    def get_Sigma(self):\n",
    "        return self.Sigma\n",
    "    \n",
    "    def likelihood(self, X, mu, Sigma):\n",
    "        T, _ = X.shape\n",
    "        B = np.zeros((T, self.N))\n",
    "        for t in range(T):\n",
    "            for i in range(self.N):\n",
    "                B[t, i]=stats.multivariate_normal(mu[i], Sigma[i]).pdf(X[t])\n",
    "        return B\n",
    "    \n",
    "    def GMM_likelihood(self, X, mu, Sigma, c):\n",
    "        T,_ = X.shape\n",
    "        B = np.zeros((3, T, self.N))\n",
    "        for t in range (T):\n",
    "            for i in range (self.N):\n",
    "                B[0][t, i]=stats.multivariate_normal(mu[0][i], Sigma[0][i]).pdf(X[t])\n",
    "                B[1][t, i]=stats.multivariate_normal(mu[1][i], Sigma[1][i]).pdf(X[t])\n",
    "                B[2][t, i]=stats.multivariate_normal(mu[2][i], Sigma[2][i]).pdf(X[t])\n",
    "        \n",
    "        prob = c[0, :]*B[0] + c[1, :]*B[1] + c[2, :]*B[2]\n",
    "        return prob, B\n",
    "    \n",
    "    def forward(self, X, pi, A, mu, Sigma, B):\n",
    "        # in this method, we will construct the forward algorithm\n",
    "        # we will use the dynamic programming to store the alpha array while computing the value\n",
    "        T, _ = X.shape\n",
    "        alpha = np.zeros((T, self.N))\n",
    "        g = np.zeros((T, ))\n",
    "        # first, we need to initialize alpha at time 0\n",
    "        for i in range(self.N):\n",
    "            alpha[0, i] = pi[i] * B[0, i]\n",
    "             #alpha[0, i] = pi[i] * get_obs_likelihood(X[0:1, :], mu, i, Sigma)\n",
    "        # normalize\n",
    "        g[0] = np.sum(alpha[0, :])\n",
    "        alpha[0:1, :] = alpha[0:1, :] / g[0]\n",
    "        # iteration\n",
    "        for t in range(1, T):\n",
    "            # for each frame\n",
    "            for j in range(self.N):\n",
    "                # for each state\n",
    "                for i in range(self.N):\n",
    "                    alpha[t, j] += alpha[t-1, i]*A[i, j]*B[t, j]\n",
    "                    # alpha[t, j] += alpha[t-1, i]*A[i, j]*get_obs_likelihood(X[t:t+1, :], mu, j, Sigma)\n",
    "            # normalize\n",
    "            g[t] = np.sum(alpha[t, :])\n",
    "            alpha[t:t+1, :] = alpha[t:t+1, :] / g[t]\n",
    "        # return alpha and g\n",
    "        return alpha, g\n",
    "    \n",
    "    def backward(self, X, pi, A, mu, Sigma, B, g):\n",
    "        T, _ = X.shape\n",
    "        beta = np.zeros((T, self.N))\n",
    "        # we use the same normalize factor as we used in forward algorithm\n",
    "        for i in range(self.N):\n",
    "            beta[T-1, i] = 1.0\n",
    "        # iteration\n",
    "        for t in range(T-2, -1, -1):\n",
    "            # for each frame\n",
    "            for i in range(self.N):\n",
    "                # for each state\n",
    "                for j in range(self.N):\n",
    "                    beta[t, i] += beta[t+1, j]*A[i, j]*B[t+1, j]\n",
    "                    # beta[t, i] += beta[t+1, j]*A[i, j]*get_obs_likelihood(X[t+1:t+2, :], mu, j, Sigma)\n",
    "            # normalize\n",
    "            beta[t:t+1, :] = beta[t:t+1, :] / g[t+1]\n",
    "        return beta\n",
    "\n",
    "#         beta = np.zeros((T, self.N))\n",
    "#         for i in range(self.N):\n",
    "#             beta[T-1, i] = 1.0\n",
    "#         for t in range(T-2, -1, -1):\n",
    "#             obs = B[t+1, :]\n",
    "#             # obs = [get_obs_likelihood(X[t+1:t+2, :], mu, j, Sigma) for j in range(self.N)]\n",
    "#             temp_obs = np.diag(obs)\n",
    "#             beta[t:t+1, :] = beta[t+1:t+2, :] @ temp_obs @ A.T / g[t+1]\n",
    "#         return beta\n",
    "    \n",
    "    def for_back(self, X, pi, A, mu, Sigma, B):\n",
    "        T, _ = X.shape\n",
    "        alpha, g = self.forward(X, pi, A, mu, Sigma, B)\n",
    "        beta = self.backward(X, pi, A, mu, Sigma, B, g)\n",
    "        gamma = np.zeros((T, self.N))\n",
    "        for t in range(T):\n",
    "            gamma[t:t+1, :] = alpha[t:t+1, :]*beta[t:t+1, :]\n",
    "            gamma[t:t+1, :] = gamma[t:t+1, :] / np.sum(gamma[t:t+1, :])\n",
    "        return alpha, beta, gamma \n",
    "    \n",
    "    \n",
    "    def find_zai(self, X, pi, A, mu, Sigma, alpha, beta, B):\n",
    "        T, _ = X.shape\n",
    "        zai = np.zeros((T, 2*self.N))\n",
    "        for t in range(T):\n",
    "            for i in range(self.N):\n",
    "                for j in range(i, i+2):\n",
    "                    zai[t, i+j] = alpha[t, i]*A[i, j]\n",
    "                    if t < T-1:\n",
    "                        if j == self.N:\n",
    "                            zai[t, i+j] = 0.0\n",
    "                        else:\n",
    "                            zai[t, i+j] *= B[t+1, j]*beta[t+1, j]\n",
    "#                     if j == self.N:\n",
    "#                         zai[t, i+j] = 0.0\n",
    "#                     elif t == T-1:\n",
    "#                         zai[t, i+j] = alpha[t, i]*A[i, j]\n",
    "#                         # zai[t, i+j] = alpha[t, i]*A[i, j]\n",
    "#                     # special case\n",
    "#                     else:\n",
    "#                         # b = get_obs_likelihood(X[t+1:t+2, :], mu, j, Sigma)\n",
    "#                         zai[t, i+j] = alpha[t, i]*A[i, j]*B[t+1, j]*beta[t+1, j]\n",
    "            # now we normalize zai_t(i, j)\n",
    "            temp_sum = np.sum(zai[t, :])\n",
    "            zai[t:t+1, :] /= temp_sum\n",
    "        return zai\n",
    "    \n",
    "    def training(self, GMM=False):\n",
    "        # in this file, we will go through all files and get all alpha beta value\n",
    "        alpha_list = []\n",
    "        beta_list = []\n",
    "        gamma_list = []\n",
    "        zai_list = []\n",
    "        B_list = []\n",
    "        for data in self.train_data:\n",
    "            if GMM:\n",
    "                B, temp_B = self.GMM_likelihood(data, self.mu, self.Sigma, self.c)\n",
    "            else:\n",
    "                B = self.likelihood(data, self.mu, self.Sigma)\n",
    "                temp_B = 0\n",
    "            temp_alpha, temp_beta, temp_gamma = self.for_back(data, self.pi, self.A, self.mu, self.Sigma, B)\n",
    "            temp_zai = self.find_zai(data, self.pi, self.A, self.mu, self.Sigma, temp_alpha, temp_beta, B)\n",
    "            alpha_list.append(temp_alpha)\n",
    "            beta_list.append(temp_beta)\n",
    "            gamma_list.append(temp_gamma)\n",
    "            zai_list.append(temp_zai)\n",
    "            B_list.append(temp_B)\n",
    "        return alpha_list, beta_list, gamma_list, zai_list, B_list\n",
    "    \n",
    "    def update(self):\n",
    "        # in this function, we will use the F/B algorithm to find alpha and beta first\n",
    "        # and use alpha and beta to update self.mu, self.Sigma and self.A\n",
    "        alpha_list, beta_list, gamma_list, zai_list, B_list = self.training(self.GMM)\n",
    "        new_A = np.zeros_like(self.A)\n",
    "        new_mu = np.zeros_like(self.mu)\n",
    "        new_Sigma = np.zeros_like(self.Sigma)\n",
    "        # first, we can sum the gamma value\n",
    "        sum_gamma = np.zeros((1, self.N))\n",
    "        for gamma in gamma_list:\n",
    "            sum_gamma += np.sum(gamma, axis=0)\n",
    "#         for i in range(self.N):\n",
    "#             temp_sum = 0\n",
    "#             for l in range(len(gamma_list)):\n",
    "#                 for t in range(gamma_list[l].shape[0]):\n",
    "#                     temp_sum += gamma_list[l][t, i]\n",
    "#             sum_gamma[i] = temp_sum\n",
    "          \n",
    "        # now we update A\n",
    "        for l in range(len(self.train_data)):\n",
    "            for t in range(self.train_data[l].shape[0]):\n",
    "                for i in range(self.N):\n",
    "                    for j in range(i, i+2):\n",
    "                        new_A[i, j] += np.sum(zai_list[l][t, i+j])/sum_gamma[0, i]\n",
    "        # new_A = new_A / sum_gamma\n",
    "            \n",
    "        # now we update mu\n",
    "        for i in range(self.N):\n",
    "            temp_mu = np.zeros((1, self.D))\n",
    "            for l in range(len(self.train_data)):\n",
    "                for t in range(self.train_data[l].shape[0]):\n",
    "                    temp_mu += self.train_data[l][t:t+1, :]* gamma_list[l][t, i]\n",
    "            temp_mu /= sum_gamma[0, i]\n",
    "            new_mu[i:i+1, :] = temp_mu\n",
    "        \n",
    "        # new we update Sigma\n",
    "        for i in range(self.N):\n",
    "            temp_Sigma = np.zeros((self.D, self.D))\n",
    "            for l in range(len(self.train_data)):\n",
    "                for t in range(self.train_data[l].shape[0]):\n",
    "                    temp = self.train_data[l][t:t+1, :] - new_mu[i:i+1, :]\n",
    "                    temp_Sigma += (temp.T @ temp) * gamma_list[l][t, i]\n",
    "            temp_Sigma /= sum_gamma[0, i]\n",
    "            new_Sigma[i, :, :] = temp_Sigma\n",
    "        \n",
    "        # update here\n",
    "        self.A = new_A\n",
    "        self.mu = new_mu\n",
    "        self.Sigma = new_Sigma\n",
    "\n",
    "    def GMM_update(self):\n",
    "        alpha_list, beta_list, gamma_list, zai_list, B_list = self.training(self.GMM)\n",
    "        new_A = np.zeros_like(self.A)\n",
    "        new_mu = np.zeros_like(self.mu)\n",
    "        new_Sigma = np.zeros_like(self.Sigma)\n",
    "        new_c = np.zeros_like(self.c)\n",
    "        sum_gamma = np.zeros((self.N, ))\n",
    "        \n",
    "        m_sum_gamma = np.zeros((self.N, ))\n",
    "        new_sum_gamma = np.zeros((3, self.N))\n",
    "        \n",
    "        # sum_gamma for A\n",
    "        for i in range(self.N):\n",
    "            temp_sum = 0\n",
    "            for l in range(len(gamma_list)):\n",
    "                for t in range(gamma_list[l].shape[0]):\n",
    "                    temp_sum += gamma_list[l][t, i]\n",
    "            sum_gamma[i] = temp_sum\n",
    "        # update new A\n",
    "        for i in range(self.N):\n",
    "            for j in range(i, i+2):\n",
    "                if j < self.N:\n",
    "                    if (j == i+1) or (j == i and j == self.N):\n",
    "                        new_A[i, j] = 1 - new_A[i, j-1]\n",
    "                    else:\n",
    "                        for l in range(len(self.train_data)):\n",
    "                            for t in range(self.train_data[l].shape[0]):\n",
    "                                new_A[i, j] += np.sum(zai_list[l][t, i+j])\n",
    "            new_A[i, :] /= sum_gamma[i]\n",
    "\n",
    "        #update new_sum_gamma\n",
    "        for l in range(len(self.train_data)):\n",
    "            for t in range(self.train_data[l].shape[0]):\n",
    "                for m in range(3):\n",
    "                    for i in range(self.N):\n",
    "                        new_sum_gamma[m,i] += gamma_list[l][t,i]*B_list[l][m][t,i]/(B_list[l][0][t,i]+B_list[l][1][t,i]+B_list[l][2][t,i])\n",
    "\n",
    "        #update the m_sum_gamma\n",
    "        for i in range(self.N):\n",
    "            m_sum_gamma[i] += np.sum(new_sum_gamma[:,i])\n",
    "\n",
    "        #updata c\n",
    "        for i in range(self.N):\n",
    "            for m in range(3):\n",
    "                new_c[m, i] = new_sum_gamma[m, i] / m_sum_gamma[i]\n",
    "\n",
    "        # update mu\n",
    "        for m in range(3):\n",
    "            for i in range(self.N):\n",
    "                temp_mu = np.zeros((1, self.D))\n",
    "                for l in range(len(self.train_data)):\n",
    "                    for t in range(self.train_data[l].shape[0]):\n",
    "                        temp_gamma = gamma_list[l][t,i]*B_list[l][m][t,i]/(B_list[l][0][t,i]+B_list[l][1][t,i]+B_list[l][2][t,i])\n",
    "                        temp_mu += self.train_data[l][t:t+1, :] * temp_gamma\n",
    "                temp_mu /= new_sum_gamma[m, i]\n",
    "                new_mu[m, i:i+1, :] = temp_mu\n",
    "\n",
    "        # update sigma\n",
    "        for m in range(3):\n",
    "            for i in range(self.N):\n",
    "                temp_Sigma = np.zeros((self.D, self.D))\n",
    "                for l in range(len(self.train_data)):\n",
    "                    for t in range(self.train_data[l].shape[0]):\n",
    "                        temp = self.train_data[l][t:t+1, :] - new_mu[m, i:i+1, :]\n",
    "                        temp_gamma = gamma_list[l][t,i]*B_list[l][m][t,i]/(B_list[l][0][t,i]+B_list[l][1][t,i]+B_list[l][2][t,i])\n",
    "                        temp_Sigma += np.outer(temp, temp)*temp_gamma\n",
    "                temp_Sigma /= new_sum_gamma[m, i]\n",
    "                new_Sigma[m, i, :, :] = temp_Sigma\n",
    "\n",
    "        self.A = new_A\n",
    "        self.mu = new_mu\n",
    "        self.Sigma = new_Sigma\n",
    "        self.c = new_c\n",
    "        \n",
    "        \n",
    "#     def training(self, data):\n",
    "#         B = self.likelihood(data, self.mu, self.Sigma)\n",
    "#         alpha, beta, gamma = self.for_back(data, self.pi, self.A, self.mu, self.Sigma, B)\n",
    "#         zai = self.find_zai(data, self.pi, self.A, self.mu, self.Sigma, alpha, beta, B)\n",
    "#         return alpha, beta, gamma, zai\n",
    "    \n",
    "#     def update(self, data):\n",
    "#         alpha, beta, gamma, zai = self.training(data)\n",
    "#         new_A = np.zeros_like(self.A)\n",
    "#         new_mu = np.zeros_like(self.mu)\n",
    "#         new_Sigma = np.zeros_like(self.Sigma)\n",
    "#         # first, we can sum the gamma value\n",
    "#         sum_gamma = np.sum(gamma, axis=0)\n",
    "\n",
    "#         # now we update A\n",
    "#         sum_zai = np.sum(zai, axis=0)\n",
    "#         for i in range(self.N):\n",
    "#             for j in range(i, i+2):\n",
    "#                 if j < self.N:\n",
    "#                     new_A[i, j] = sum_zai[i+j]\n",
    "#             new_A[i, :] /= sum_gamma[i]\n",
    "            \n",
    "#         # now we update mu\n",
    "#         for i in range(self.N):\n",
    "#             temp_mu = np.zeros((1, self.D))\n",
    "#             for t in range(data.shape[0]):\n",
    "#                 temp_mu += data[t:t+1, :] * gamma[t, i]\n",
    "#             temp_mu /= sum_gamma[i]\n",
    "#             new_mu[i:i+1, :] = temp_mu        \n",
    "        \n",
    "#         # new we update Sigma\n",
    "#         for i in range(self.N):\n",
    "#             temp_Sigma = np.zeros((self.D, self.D))\n",
    "#             for t in range(data.shape[0]):\n",
    "#                 temp = data[t:t+1, :] - new_mu[i:i+1, :]\n",
    "#                 temp_Sigma += np.outer(temp, temp) * gamma[t, i]\n",
    "#             temp_Sigma /= sum_gamma[i]\n",
    "#             # deal with singular matrix\n",
    "#             if la.det(temp_Sigma) < 1e-10:\n",
    "#                 temp_Sigma += 0.5*np.eye(self.D)\n",
    "#                 # print(la.det(temp_Sigma))\n",
    "#             new_Sigma[i] = temp_Sigma\n",
    "        \n",
    "#         # update here\n",
    "#         self.A = new_A\n",
    "#         self.mu = new_mu\n",
    "#         self.Sigma = new_Sigma\n",
    "        \n",
    "    def iterate(self):\n",
    "        # in this function, we will iterate through all file\n",
    "        for i in range(10):\n",
    "            if self.GMM:\n",
    "                self.GMM_update()\n",
    "            else:\n",
    "                self.update()\n",
    "          \n",
    "    def test(self, test_data):\n",
    "        # in this method, we will test on the test input data\n",
    "        retval = np.zeros((len(test_data, )))\n",
    "        for i, data in enumerate(test_data):\n",
    "            if self.GMM:\n",
    "                B, _ = self.GMM_likelihood(data, self.mu, self.Sigma, self.c)\n",
    "            else:\n",
    "                B = self.likelihood(data, self.mu, self.Sigma)\n",
    "            # print('in test data', i, \"likelihood of first frame is\", B[0, :])\n",
    "            alpha, g = self.forward(data, self.pi, self.A, self.mu, self.Sigma, B)\n",
    "            retval[i] = np.sum(np.log(g))\n",
    "        return retval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = ['asr', 'cnn', 'dnn', 'hmm', 'tts']\n",
    "def final_test(words, test_dict, model_asr, model_cnn, model_dnn, model_hmm, model_tts):\n",
    "    all_decisions = []\n",
    "    for word in words:\n",
    "        single_decision = []\n",
    "        test_data = test_dict[word]\n",
    "        temp_decision = np.zeros((5, len(test_data)))\n",
    "        temp_decision[0, :] = model_asr.test(test_data)\n",
    "        temp_decision[1, :] = model_cnn.test(test_data)\n",
    "        temp_decision[2, :] = model_dnn.test(test_data)\n",
    "        temp_decision[3, :] = model_hmm.test(test_data)\n",
    "        temp_decision[4, :] = model_tts.test(test_data)\n",
    "        for i in range(len(test_data)):\n",
    "            single_decision.append(np.argmax(temp_decision[:,i]))\n",
    "        all_decisions.append(single_decision)\n",
    "    return all_decisions\n",
    "\n",
    "def accuracy(decisions):\n",
    "    acc_mat = np.zeros((5, 5))\n",
    "    for i in range(len(decision)):\n",
    "        for j in range(len(decision[i])):\n",
    "            acc_mat[i, decision[i][j]] += 1\n",
    "        acc_mat[i:i+1, :] /= np.sum(acc_mat[i:i+1, :])\n",
    "    acc = np.trace(acc_mat) / 5\n",
    "    return acc_mat, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the speaker dependent test\n",
    "At last of the test, we print the decision matrix we have and the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0], [1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "the accuracy of the speaker dependent experiment is 100.0 %\n"
     ]
    }
   ],
   "source": [
    "cnn_train_data = pd_dict['train']['cnn']\n",
    "dnn_train_data = pd_dict['train']['dnn']\n",
    "asr_train_data = pd_dict['train']['asr']\n",
    "hmm_train_data = pd_dict['train']['hmm']\n",
    "tts_train_data = pd_dict['train']['tts']\n",
    "# train data\n",
    "pd_model_asr = My_HMM(asr_train_data)\n",
    "pd_model_cnn = My_HMM(cnn_train_data)\n",
    "pd_model_dnn = My_HMM(dnn_train_data)\n",
    "pd_model_hmm = My_HMM(hmm_train_data)\n",
    "pd_model_tts = My_HMM(tts_train_data)\n",
    "\n",
    "pd_model_asr.iterate()\n",
    "pd_model_cnn.iterate()\n",
    "pd_model_dnn.iterate()\n",
    "pd_model_hmm.iterate()\n",
    "pd_model_tts.iterate()\n",
    "\n",
    "test_dict = pd_dict['test']\n",
    "decision = final_test(words, test_dict, pd_model_asr, pd_model_cnn, pd_model_dnn, pd_model_hmm, pd_model_tts)\n",
    "print(decision)\n",
    "acc_mat, total_acc = accuracy(decision)\n",
    "print(acc_mat)\n",
    "print('the accuracy of the speaker dependent experiment is', total_acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the speaker independent test\n",
    "At last of the test, we print the decision matrix we have and the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0], [0, 1, 1, 1, 1], [3, 3, 3, 3, 3], [3, 3, 3, 3, 3], [4, 4, 4, 4, 4]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.2 0.8 0.  0.  0. ]\n",
      " [0.  0.  0.  1.  0. ]\n",
      " [0.  0.  0.  1.  0. ]\n",
      " [0.  0.  0.  0.  1. ]]\n",
      "the accuracy of the speaker independent experiment is 76.0 %\n"
     ]
    }
   ],
   "source": [
    "cnn_train_data = pid_dict['train']['cnn']\n",
    "dnn_train_data = pid_dict['train']['dnn']\n",
    "asr_train_data = pid_dict['train']['asr']\n",
    "hmm_train_data = pid_dict['train']['hmm']\n",
    "tts_train_data = pid_dict['train']['tts']\n",
    "# train data\n",
    "pid_model_asr = My_HMM(asr_train_data)\n",
    "pid_model_cnn = My_HMM(cnn_train_data)\n",
    "pid_model_dnn = My_HMM(dnn_train_data)\n",
    "pid_model_hmm = My_HMM(hmm_train_data)\n",
    "pid_model_tts = My_HMM(tts_train_data)\n",
    "\n",
    "pid_model_asr.iterate()\n",
    "pid_model_cnn.iterate()\n",
    "pid_model_dnn.iterate()\n",
    "pid_model_hmm.iterate()\n",
    "pid_model_tts.iterate()\n",
    "\n",
    "test_dict = pid_dict['test']\n",
    "decision = final_test(words, test_dict, pid_model_asr, pid_model_cnn, pid_model_dnn, pid_model_hmm, pid_model_tts)\n",
    "print(repr(decision))\n",
    "acc_mat, total_acc = accuracy(decision)\n",
    "print(acc_mat)\n",
    "print('the accuracy of the speaker independent experiment is', total_acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User record word test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [3, 3, 3, 3, 3], [4, 4, 4, 4, 4]]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "the accuracy of the user input experiment is 60.0 %\n"
     ]
    }
   ],
   "source": [
    "# use the speaker independent model\n",
    "decision = final_test(words, dd_dict, pid_model_asr, pid_model_cnn, pid_model_dnn, pid_model_hmm, pid_model_tts)\n",
    "print(repr(decision))\n",
    "acc_mat, total_acc = accuracy(decision)\n",
    "print(acc_mat)\n",
    "print('the accuracy of the user input experiment is', total_acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
